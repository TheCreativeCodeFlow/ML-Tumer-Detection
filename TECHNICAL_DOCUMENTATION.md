# Medical Tumor Detection System - Technical Documentation

## üìö Table of Contents
1. [System Overview](#system-overview)
2. [Architecture](#architecture)
3. [Component Details](#component-details)
4. [Data Flow](#data-flow)
5. [AI Models](#ai-models)
6. [Image Processing Pipeline](#image-processing-pipeline)
7. [Database Schema](#database-schema)
8. [API Reference](#api-reference)
9. [Deployment Guide](#deployment-guide)
10. [Troubleshooting](#troubleshooting)

---

## üéØ System Overview

The **Medical Tumor Detection System** is a full-stack AI-powered web application designed for analyzing medical images to detect tumors, fractures, hemorrhages, and other abnormalities. Built with Python and Streamlit, it leverages state-of-the-art deep learning models from Hugging Face to provide clinical decision support.

### Key Capabilities
- ‚úÖ **Multi-Modal Analysis**: Supports X-ray, CT, MRI, and DICOM formats
- ‚úÖ **4 Specialized AI Models**: Each optimized for specific medical conditions
- ‚úÖ **Explainable AI**: Grad-CAM visualization showing model attention
- ‚úÖ **Clinical Workflow Integration**: Notes, feedback, and audit trails
- ‚úÖ **Professional Reporting**: Automated PDF report generation
- ‚úÖ **Historical Analytics**: Trend analysis and performance metrics

---

## üèóÔ∏è Architecture

### High-Level Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        User Interface                        ‚îÇ
‚îÇ                      (Streamlit Web App)                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Application Layer                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ   app.py     ‚îÇ  ‚îÇ  Navigation  ‚îÇ  ‚îÇ  Dashboard   ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  (Main App)  ‚îÇ  ‚îÇ   Router     ‚îÇ  ‚îÇ   Builder    ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Business Logic                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇmodel_handler ‚îÇ  ‚îÇ   utils.py   ‚îÇ  ‚îÇ PDF Generator‚îÇ     ‚îÇ
‚îÇ  ‚îÇ    .py       ‚îÇ  ‚îÇ (Preprocessing)‚îÇ  ‚îÇ              ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      AI/ML Layer                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ HuggingFace  ‚îÇ  ‚îÇ   PyTorch    ‚îÇ  ‚îÇ  Grad-CAM    ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ Transformers ‚îÇ  ‚îÇ    Models    ‚îÇ  ‚îÇVisualization ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Data Layer                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ   SQLite     ‚îÇ  ‚îÇ  Image Files ‚îÇ  ‚îÇ  PDF Reports ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ   Database   ‚îÇ  ‚îÇ   (Cache)    ‚îÇ  ‚îÇ   (Export)   ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Technology Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Frontend** | Streamlit | Web UI framework |
| **Backend** | Python 3.10+ | Application logic |
| **AI/ML** | PyTorch + Transformers | Deep learning inference |
| **Database** | SQLite | Analysis history storage |
| **Visualization** | Plotly + Matplotlib | Charts and graphs |
| **Reports** | ReportLab | PDF generation |
| **Medical Imaging** | PyDICOM + Pillow | Image processing |

---

## üîß Component Details

### 1. Main Application (`app.py`)

**File Size**: 1009 lines  
**Purpose**: Orchestrates entire application flow

#### Key Functions:

##### **A. Configuration & Setup**
```python
st.set_page_config(
    page_title="Tumor Detection (Medical Images)", 
    layout="wide", 
    initial_sidebar_state="expanded"
)
```
- Configures Streamlit page settings
- Sets up wide layout for medical imaging
- Initializes dark mode theme

##### **B. Model Selection System**
```python
MODEL_OPTIONS = {
    "Bone Fracture Detection (X-ray)": "Hemgg/bone-fracture-detection-using-xray",
    "Intracranial Hemorrhage Detection": "DifeiT/rsna-intracranial-hemorrhage-detection",
    "Brain Tumor Detection": "ShimaGh/Brain-Tumor-Detection",
    "MRI Brain Abnormality Classification": "hugginglearners/brain-tumor-detection-mri"
}
```
**How it works:**
1. User selects model from dropdown
2. Model name stored in session state
3. Lazy loading when first prediction requested
4. Cached for subsequent uses

##### **C. Navigation Router**
```python
page = st.radio("Navigation", ["Home", "Upload & Detect", "Model Insights", "About AI Model"])
```
- **Home**: Landing page with instructions
- **Upload & Detect**: Main analysis interface
- **Model Insights**: Historical data and metrics
- **About AI Model**: Technical documentation

##### **D. Results Dashboard** (`create_results_dashboard()`)
**Inputs:**
- Original image
- Grad-CAM visualization
- Prediction & confidence
- Model metadata

**Outputs:**
- Summary metrics cards
- Side-by-side image comparison
- Confidence distribution chart
- Medical interpretation guide
- Clinical feedback buttons
- Notes input area

**Severity Assessment Logic:**
```python
if "fracture" in pred.lower():
    severity = "High Risk" if conf > 0.8 else "Moderate Risk" if conf > 0.6 else "Low Risk"
elif "hemorrhage" in pred.lower():
    severity = "Critical" if conf > 0.8 else "Urgent" if conf > 0.6 else "Monitor"
elif "tumor" in pred.lower():
    severity = "High Suspicion" if conf > 0.8 else "Moderate Suspicion" if conf > 0.6 else "Low Suspicion"
```

##### **E. History Management** (`create_history_view()`)
**Features:**
- Filter by model, prediction, confidence
- Display data in sortable table
- Visualizations:
  - Prediction distribution (pie chart)
  - Confidence trends (line chart)
- Bulk delete operations
- CSV export functionality

##### **F. PDF Report Generation**

**Two types of reports:**

1. **Individual Report** (`generate_detailed_pdf_report()`)
   - Analysis ID and timestamp
   - Patient information (optional)
   - Prediction with confidence
   - Risk assessment
   - Clinical interpretation
   - Technical details
   - Medical disclaimers

2. **Summary Report** (`generate_summary_pdf_report()`)
   - Executive summary statistics
   - Prediction breakdown table
   - Confidence statistics
   - Visual analytics charts
   - Recent analyses summary
   - Comprehensive disclaimers

---

### 2. Model Handler (`model_handler.py`)

**File Size**: 226 lines  
**Purpose**: AI model management and inference

#### Class Structure:

```python
class ModelHandler:
    def __init__(self, model_name: str)
    def _load(self)
    def _create_default_label_map(self)
    def predict(self, input_tensor: np.ndarray)
    def gradcam(self, input_tensor: np.ndarray)
    def interpret_label(self, label: str, probs: np.ndarray)
```

#### Detailed Workflow:

##### **A. Initialization**
```python
def __init__(self, model_name: str):
    self.model_name = model_name
    self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    self.input_size = (224, 224)
    self.model = None
    self.extractor = None
    self.label_map = None
    self._load()
```
**Device Selection:**
- Checks for CUDA GPU availability
- Falls back to CPU if GPU not available
- Logs device information

##### **B. Model Loading** (`_load()`)

**3-Tier Loading Strategy:**

1. **Primary: HuggingFace Transformers**
```python
try:
    self.extractor = AutoFeatureExtractor.from_pretrained(self.model_name)
    self.model = AutoModelForImageClassification.from_pretrained(
        self.model_name,
        trust_remote_code=True
    )
    self.model.to(self.device)
    self.model.eval()
```
- Downloads model from Hugging Face Hub
- Loads feature extractor/preprocessor
- Sets model to evaluation mode
- Moves to GPU/CPU

2. **Fallback: timm (PyTorch Image Models)**
```python
except Exception:
    model_name = 'resnet50'  # or 'vit_base_patch16_224', 'efficientnet_b0'
    self.model = timm.create_model(model_name, pretrained=True, num_classes=1000)
```
- Uses standard computer vision architectures
- Pre-trained on ImageNet
- General purpose fallback

3. **Last Resort: Dummy Model**
```python
self.model = nn.Sequential(
    nn.AdaptiveAvgPool2d((1, 1)),
    nn.Flatten(),
    nn.Linear(3, 2),
    nn.Softmax(dim=1)
)
```
- Simple neural network for testing
- Returns mock predictions
- Prevents application crashes

##### **C. Prediction Pipeline** (`predict()`)

**Step-by-Step Process:**

```python
def predict(self, input_tensor: np.ndarray):
    # Step 1: Convert to PyTorch tensor
    x = torch.tensor(input_tensor, dtype=torch.float32).to(self.device)
    
    # Step 2: Inference (no gradient computation)
    with torch.no_grad():
        outputs = self.model(x)
        logits = outputs.logits if hasattr(outputs, 'logits') else outputs
        
    # Step 3: Apply softmax to get probabilities
    probs = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy()[0]
    
    # Step 4: Get predicted class
    idx = int(np.argmax(probs))
    conf = float(probs[idx])
    
    # Step 5: Map to label
    label = self.label_map[idx] if idx in self.label_map else str(idx)
    
    # Step 6: Interpret for medical context
    medical_label, medical_conf = self.interpret_label(label, probs)
    
    return medical_label, conf, probs
```

**Error Handling:**
```python
except Exception as e:
    print(f"‚ùå Prediction error: {e}")
    return "Error in prediction", 0.0, np.array([0.0])
```

##### **D. Grad-CAM Visualization** (`gradcam()`)

**Purpose**: Show where the AI is "looking" in the image

**Process:**

```python
def gradcam(self, input_tensor: np.ndarray):
    # Step 1: Prepare display image
    img = input_tensor[0].transpose(1, 2, 0)
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    img_disp = (img * std) + mean
    img_disp = np.clip(img_disp, 0, 1)
    
    # Step 2: Find target layer
    if hasattr(self.model, 'layer4'):
        target_layers = [self.model.layer4[-1]]  # ResNet
    elif hasattr(self.model, 'blocks'):
        target_layers = [self.model.blocks[-1]]  # Vision Transformer
    elif hasattr(self.model, 'stages'):
        target_layers = [self.model.stages[-1]]  # EfficientNet
    
    # Step 3: Create Grad-CAM
    cam = GradCAM(model=self.model, target_layers=target_layers)
    
    # Step 4: Generate heatmap
    _, conf, probs = self.predict(input_tensor)
    idx = int(np.argmax(probs))
    targets = [ClassifierOutputTarget(idx)]
    grayscale_cam = cam(input_tensor_t, targets=targets)[0]
    
    # Step 5: Overlay on image
    visualization = show_cam_on_image(img_disp, grayscale_cam, use_rgb=True)
    
    return visualization
```

**Grad-CAM Explained:**
- Computes gradient of predicted class w.r.t. feature maps
- Weights feature maps by gradient importance
- Creates heatmap showing important regions
- Red = High attention, Blue = Low attention

##### **E. Medical Interpretation** (`interpret_label()`)

**Model-Specific Logic:**

```python
def interpret_label(self, label: str, probs: np.ndarray):
    l = str(label).lower()
    
    # Bone Fracture Model
    if "bone-fracture" in self.model_name.lower():
        if any(k in l for k in ['fracture', 'break', 'crack', '1']):
            severity = "High" if probs.max() > 0.8 else "Moderate" if probs.max() > 0.6 else "Low"
            return f'Bone Fracture Detected ({severity} Confidence)', probs.max()
        else:
            return 'No Fracture Detected', probs.max()
    
    # Hemorrhage Model
    elif "hemorrhage" in self.model_name.lower():
        if any(k in l for k in ['hemorrhage', 'bleeding', '1']):
            severity = "Critical" if probs.max() > 0.8 else "Moderate" if probs.max() > 0.6 else "Mild"
            return f'Intracranial Hemorrhage ({severity} Risk)', probs.max()
        else:
            return 'No Hemorrhage Detected', probs.max()
    
    # Tumor Model
    elif "tumor" in self.model_name.lower():
        if any(k in l for k in ['tumor', 'mass', 'lesion', '1', 'positive']):
            severity = "High Suspicion" if probs.max() > 0.8 else "Moderate Suspicion" if probs.max() > 0.6 else "Low Suspicion"
            return f'Brain Tumor Detected ({severity})', probs.max()
        else:
            return 'No Tumor Detected', probs.max()
```

---

### 3. Image Processing Utilities (`utils.py`)

**File Size**: 102 lines  
**Purpose**: Image loading and preprocessing

#### Functions:

##### **A. File Validation** (`allowed_file()`)
```python
ALLOWED_EXT = ('.png', '.jpg', '.jpeg', '.dcm')

def allowed_file(filename: str) -> bool:
    fn = filename.lower()
    return any(fn.endswith(ext) for ext in ALLOWED_EXT)
```

##### **B. Image Loading** (`load_image()`)

**Handles Multiple Formats:**

1. **DICOM Files (.dcm)**
```python
if fn.endswith('.dcm'):
    ds = pydicom.dcmread(file_buffer)
    arr = ds.pixel_array
    
    # Normalize to 0-255
    arr = arr.astype('float32')
    arr -= arr.min()
    if arr.max() > 0:
        arr /= arr.max()
    arr = (arr * 255).astype('uint8')
    
    # Convert to RGB
    if arr.ndim == 2:
        img = Image.fromarray(arr).convert('RGB')
```

2. **Standard Images (.jpg, .png)**
```python
else:
    img = Image.open(file_buffer)
    if img.mode != 'RGB':
        img = img.convert('RGB')
```

##### **C. Image Preprocessing** (`preprocess_image()`)

**Complete Pipeline:**

```python
def preprocess_image(img: Image.Image, target_size=(224, 224)):
    # Step 1: Handle EXIF orientation
    img = ImageOps.exif_transpose(img)
    
    # Step 2: Resize with aspect ratio preservation
    img = ImageOps.fit(img, target_size, Image.Resampling.LANCZOS)
    
    # Step 3: Convert to numpy array
    arr = np.array(img).astype('float32') / 255.0
    
    # Step 4: Apply ImageNet normalization
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    arr = (arr - mean) / std
    
    # Step 5: Transpose HWC to CHW
    arr = np.transpose(arr, (2, 0, 1))
    
    # Step 6: Add batch dimension
    return arr[np.newaxis, ...]
```

**Why ImageNet Normalization?**
- Pre-trained models expect this normalization
- Mean and std calculated from ImageNet dataset
- Ensures consistent input distribution

---

## üîÑ Data Flow

### Complete Request-Response Cycle

```
1. User Action: Upload Image
   ‚Üì
2. File Upload: uploaded = st.file_uploader(...)
   ‚Üì
3. File Validation: allowed_file(uploaded.name)
   ‚Üì
4. Read Binary Data: file_bytes = uploaded.read()
   ‚Üì
5. Load Image: img = load_image(io.BytesIO(file_bytes), uploaded.name)
   ‚Üì
6. Preprocess: input_tensor = preprocess_image(img, handler.input_size)
   ‚Üì
7. Model Inference: pred, conf, probs = handler.predict(input_tensor)
   ‚Üì
8. Grad-CAM: cam_img = handler.gradcam(input_tensor)
   ‚Üì
9. Save to Database: conn.execute("INSERT INTO results ...")
   ‚Üì
10. Display Results: create_results_dashboard(...)
   ‚Üì
11. Generate Report (Optional): generate_detailed_pdf_report(...)
```

### State Management

**Streamlit Session State:**
```python
st.session_state.model_name  # Current selected model
st.session_state.dark         # Theme preference
```

**Database Persistence:**
```python
# Results table
timestamp, model, filename, prediction, confidence, true_label

# Feedback table
timestamp, feedback_type, created_at

# Notes table
timestamp, notes, created_at
```

---

## üß† AI Models

### Model 1: Bone Fracture Detection
- **Model ID**: `Hemgg/bone-fracture-detection-using-xray`
- **Architecture**: CNN-based (likely ResNet or EfficientNet)
- **Input Size**: 224√ó224 RGB
- **Output**: Binary classification (Fracture/No Fracture)
- **Training Data**: X-ray bone images
- **Use Cases**: Emergency departments, orthopedics, sports medicine

**Performance Thresholds:**
- High Risk: Confidence > 80%
- Moderate Risk: 60% < Confidence ‚â§ 80%
- Low Risk: Confidence ‚â§ 60%

### Model 2: Intracranial Hemorrhage Detection
- **Model ID**: `DifeiT/rsna-intracranial-hemorrhage-detection`
- **Architecture**: Transformer-based (ViT or DeiT)
- **Input Size**: 224√ó224 RGB
- **Output**: Binary classification (Hemorrhage/No Hemorrhage)
- **Training Data**: RSNA Intracranial Hemorrhage Detection dataset
- **Use Cases**: Emergency neurology, stroke assessment, ICU monitoring

**Risk Levels:**
- Critical: Confidence > 80%
- Urgent: 60% < Confidence ‚â§ 80%
- Monitor: Confidence ‚â§ 60%

### Model 3: Brain Tumor Detection
- **Model ID**: `ShimaGh/Brain-Tumor-Detection`
- **Architecture**: CNN-based
- **Input Size**: 224√ó224 RGB
- **Output**: Binary classification (Tumor/No Tumor)
- **Training Data**: Brain MRI scans
- **Use Cases**: Oncology screening, neurosurgery planning, follow-up monitoring

**Suspicion Levels:**
- High Suspicion: Confidence > 80%
- Moderate Suspicion: 60% < Confidence ‚â§ 80%
- Low Suspicion: Confidence ‚â§ 60%

### Model 4: MRI Brain Abnormality Classification
- **Model ID**: `hugginglearners/brain-tumor-detection-mri`
- **Architecture**: CNN-based
- **Input Size**: 224√ó224 RGB
- **Output**: Multi-class classification
- **Training Data**: Brain MRI dataset
- **Use Cases**: General brain imaging, research, educational purposes

---

## üñºÔ∏è Image Processing Pipeline

### Input Formats Supported

| Format | Extension | Use Case |
|--------|-----------|----------|
| JPEG | .jpg, .jpeg | Standard medical images |
| PNG | .png | Lossless medical images |
| DICOM | .dcm | Medical imaging standard (CT, MRI, X-ray) |

### Processing Steps

1. **Loading**
   - DICOM: Extract pixel array, normalize, convert to RGB
   - JPEG/PNG: Load with PIL, ensure RGB mode

2. **Preprocessing**
   - EXIF orientation correction
   - Resize to 224√ó224 (maintaining aspect ratio)
   - Center crop if needed
   - Normalize to [0, 1]
   - Apply ImageNet statistics

3. **Tensor Conversion**
   - Convert to float32
   - Transpose from HWC to CHW
   - Add batch dimension (1, 3, 224, 224)

### Post-Processing

1. **Grad-CAM Generation**
   - Compute gradients
   - Generate attention heatmap
   - Overlay on original image

2. **Visualization**
   - Unnormalize image
   - Apply colormap (red-yellow-green)
   - Resize for display

---

## üíæ Database Schema

### Results Table
```sql
CREATE TABLE results (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp TEXT NOT NULL,
    model TEXT NOT NULL,
    filename TEXT NOT NULL,
    prediction TEXT NOT NULL,
    confidence REAL NOT NULL,
    true_label TEXT
);
```

**Indexes:**
```sql
CREATE INDEX idx_timestamp ON results(timestamp);
CREATE INDEX idx_model ON results(model);
CREATE INDEX idx_prediction ON results(prediction);
```

### Feedback Table
```sql
CREATE TABLE feedback (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp TEXT NOT NULL,
    feedback_type TEXT NOT NULL,
    created_at TEXT NOT NULL
);
```

**Feedback Types:**
- `confirmed`: Clinically validated
- `disagreement`: Clinical disagreement
- `followup_required`: Requires additional imaging

### Notes Table
```sql
CREATE TABLE notes (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp TEXT NOT NULL,
    notes TEXT NOT NULL,
    created_at TEXT NOT NULL
);
```

---

## üì° API Reference

### Main Functions

#### `get_handler(name: str) -> ModelHandler`
```python
@st.cache_resource
def get_handler(name):
    return ModelHandler(name)
```
**Purpose**: Load and cache AI model  
**Parameters**: Model name (Hugging Face ID)  
**Returns**: ModelHandler instance  
**Caching**: Resource cached across sessions

#### `create_results_dashboard(...)`
```python
def create_results_dashboard(
    img: Image.Image,
    cam_img: np.ndarray,
    pred: str,
    conf: float,
    probs: np.ndarray,
    handler: ModelHandler,
    timestamp: str,
    patient_id: str
) -> None
```
**Purpose**: Display comprehensive analysis results  
**Side Effects**: Renders Streamlit UI components

#### `generate_detailed_pdf_report(analysis_id: int, conn) -> bytes`
```python
def generate_detailed_pdf_report(analysis_id, conn):
    # Returns PDF binary data
    return buffer.getvalue()
```
**Purpose**: Generate individual analysis PDF report  
**Returns**: PDF file as bytes  
**Database**: Reads from results table

---

## üöÄ Deployment Guide

### Local Deployment

1. **Clone Repository**
```bash
git clone https://github.com/TheCreativeCodeFlow/ML-Tumer-Detection.git
cd ML-Tumer-Detection
```

2. **Create Virtual Environment**
```bash
python -m venv .venv
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/Mac
```

3. **Install Dependencies**
```bash
pip install -r requirements.txt
```

4. **Run Application**
```bash
streamlit run app.py
```

5. **Access Application**
- Local: http://localhost:8501
- Network: http://<your-ip>:8501

### Docker Deployment

1. **Build Image**
```bash
docker build -t medical-tumor-detection .
```

2. **Run Container**
```bash
docker run -p 8501:8501 medical-tumor-detection
```

3. **With GPU Support**
```bash
docker run --gpus all -p 8501:8501 medical-tumor-detection
```

### Cloud Deployment

#### Streamlit Cloud
1. Push code to GitHub
2. Connect repository to Streamlit Cloud
3. Deploy with one click

#### Hugging Face Spaces
1. Create new Space
2. Upload files
3. Configure requirements
4. Deploy

#### AWS/Azure/GCP
1. Use Docker container
2. Deploy to container service
3. Configure load balancer
4. Set up SSL certificate

---

## üîç Troubleshooting

### Common Issues

#### Issue 1: Model Loading Error
**Symptoms**: "Error in prediction" with 0.0% confidence

**Causes:**
- Model not found on Hugging Face
- Network connectivity issues
- Insufficient memory
- CUDA compatibility issues

**Solutions:**
```python
# Check model availability
from transformers import AutoModelForImageClassification
model = AutoModelForImageClassification.from_pretrained(
    "model-name",
    trust_remote_code=True
)

# Check GPU availability
import torch
print(torch.cuda.is_available())
print(torch.cuda.get_device_name(0))
```

#### Issue 2: DICOM Loading Error
**Symptoms**: "pydicom not installed" error

**Solution:**
```bash
pip install pydicom
```

#### Issue 3: PDF Generation Error
**Symptoms**: PDF reports not generating

**Solution:**
```bash
pip install reportlab matplotlib
```

#### Issue 4: Out of Memory
**Symptoms**: CUDA out of memory error

**Solutions:**
- Reduce batch size (already set to 1)
- Use CPU instead of GPU
- Reduce model size
- Close other applications

#### Issue 5: Grad-CAM Not Working
**Symptoms**: Original image shown instead of heatmap

**Causes:**
- Incompatible model architecture
- pytorch-grad-cam not installed

**Solution:**
```bash
pip install pytorch-grad-cam
```

### Debug Mode

**Enable detailed logging:**
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

**Check model configuration:**
```python
print(handler.model.config)
print(handler.label_map)
print(handler.device)
```

---

## üìä Performance Optimization

### Model Caching
- Models cached with `@st.cache_resource`
- Loaded once per application lifetime
- Reduces loading time from ~30s to instant

### GPU Acceleration
- Automatically uses CUDA if available
- ~10-100x faster inference than CPU
- Batch processing possible (currently disabled)

### Database Indexing
```sql
CREATE INDEX idx_timestamp ON results(timestamp);
CREATE INDEX idx_model ON results(model);
```

### Image Preprocessing Optimization
- LANCZOS resampling for quality
- Vectorized numpy operations
- Minimal memory allocation

---

## üîê Security Considerations

### Input Validation
- File extension whitelist
- File size limits (configure in Streamlit)
- MIME type checking

### Data Privacy
- Local SQLite database (no cloud storage)
- Optional patient ID (can be anonymous)
- No data transmission to external servers (except model downloads)

### Medical Disclaimer
```
‚ö†Ô∏è For Research and Educational Purposes Only

This AI system is designed to assist healthcare professionals
and researchers, not replace clinical judgment. All predictions
require clinical validation and professional medical interpretation.
```

---

## üìà Future Enhancements

### Planned Features
1. **Batch Processing**: Analyze multiple images simultaneously
2. **DICOM Viewer**: Integrated medical image viewer
3. **Model Fine-tuning**: Custom model training interface
4. **API Endpoints**: REST API for programmatic access
5. **Real-time Monitoring**: Dashboard for model performance
6. **Multi-language Support**: Internationalization
7. **Mobile App**: Native mobile application
8. **Cloud Integration**: AWS/Azure medical imaging services

### Research Opportunities
- Model ensemble methods
- Uncertainty quantification
- Active learning integration
- Federated learning support
- 3D volume analysis

---

## üìö References

### Libraries Documentation
- [Streamlit](https://docs.streamlit.io/)
- [PyTorch](https://pytorch.org/docs/)
- [Transformers](https://huggingface.co/docs/transformers/)
- [timm](https://timm.fast.ai/)
- [Grad-CAM](https://github.com/jacobgil/pytorch-grad-cam)

### Medical Imaging Standards
- [DICOM](https://www.dicomstandard.org/)
- [HL7 FHIR](https://www.hl7.org/fhir/)

### Research Papers
- Grad-CAM: Visual Explanations from Deep Networks
- Deep Learning for Medical Image Analysis
- Transformers in Medical Imaging

---

## üìû Support

For technical issues or questions:
1. Check this documentation
2. Review CHANGELOG.md for recent updates
3. Check MEDICAL_MODELS_GUIDE.md for model-specific info
4. Run validate_models.py for diagnostics

---

**Version**: 2.0  
**Last Updated**: October 8, 2025  
**Status**: Production Ready ‚úÖ  
**License**: MIT  
**Maintainer**: TheCreativeCodeFlow
